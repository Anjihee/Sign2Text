import cv2
import mediapipe as mp
import numpy as np
import joblib
from tensorflow.keras.models import load_model
from PIL import ImageFont, ImageDraw, Image

#  í•œê¸€ í°íŠ¸ ê²½ë¡œ (macOS ê¸°ë³¸)
font_path = "/System/Library/Fonts/Supplemental/AppleGothic.ttf"
font = ImageFont.truetype(font_path, 32)

# ëª¨ë¸ ë° ì¸ì½”ë” ë¶ˆëŸ¬ì˜¤ê¸°
model = load_model("../models/cnn1d_model.h5")
label_encoder = joblib.load("../models/label_encoder.pkl")

# MediaPipe ì† ì¸ì‹ ì´ˆê¸°í™” (ì–‘ì†)
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.7)
mp_drawing = mp.solutions.drawing_utils

# ì–‘ì† ì¢Œí‘œ ì¶”ì¶œ í•¨ìˆ˜
def extract_dual_hand_landmarks(results):
    coords_l = np.zeros((21, 2))  # ì™¼ì†
    coords_r = np.zeros((21, 2))  # ì˜¤ë¥¸ì†

    if not results.multi_hand_landmarks or not results.multi_handedness:
        return None

    for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
        label = handedness.classification[0].label  # 'Left' or 'Right'
        coords = np.array([[lm.x, lm.y] for lm in hand_landmarks.landmark])
        if label == 'Left':
            coords_l = coords
        elif label == 'Right':
            coords_r = coords

    # ì¢Œìš° ì† ì¢Œí‘œ ì—°ê²° â†’ (84,) ë²¡í„°
    full_coords = np.concatenate([coords_l.flatten(), coords_r.flatten()])
    full_coords = full_coords.astype(np.float32)
    return full_coords.reshape(1, 84, 1)

# ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì‹œì‘
cap = cv2.VideoCapture(0)
print("ğŸ“· ì–‘ì† ê¸°ë°˜ ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ ì‹œì‘ (ì¢…ë£Œ: Q í‚¤)")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    image = cv2.flip(frame, 1)
    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)

    if results.multi_hand_landmarks:
        # ì† ëœë“œë§ˆí¬ ì‹œê°í™”
        for hand_landmarks in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        coords = extract_dual_hand_landmarks(results)
        if coords is not None:
            prediction = model.predict(coords, verbose=0)
            predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])[0]
            confidence = np.max(prediction)

            #  í•œê¸€ ìë§‰ ì¶œë ¥ (PIL ì‚¬ìš©)
            img_pil = Image.fromarray(image)
            draw = ImageDraw.Draw(img_pil)
            draw.text((10, 30), f"{predicted_label} ({confidence:.2f})", font=font, fill=(0, 255, 0))
            image = np.array(img_pil)

    cv2.imshow("Sign2Text ì–‘ì† Inference", image)
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()