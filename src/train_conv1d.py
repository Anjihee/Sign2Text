# src/train_conv1d.py

import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, LSTM
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
import joblib

# ğŸ“ ê²½ë¡œ ì„¤ì •
DATA_PATH = os.path.join("..", "dataset", "filtered_sentence_data.csv")
MODEL_SAVE_PATH = os.path.join("..", "models", "cnn1d_model.h5")
ENCODER_SAVE_PATH = os.path.join("..", "models", "label_encoder.pkl")

# ğŸ“‚ 1. ë°ì´í„° ë¡œë”©
df = pd.read_csv(DATA_PATH)

# ğŸ§¼ 2. X, y ë¶„ë¦¬ ë° ì „ì²˜ë¦¬
drop_cols = ["label"]
if "word_id" in df.columns:
    drop_cols.append("word_id")

X = df.drop(columns=drop_cols).astype(np.float32).values
X = np.nan_to_num(X)
X = X.reshape(-1, 84)  # 21 keypoints * 2 hands * (x, y)

# âœ… ì •ê·œí™” í•¨ìˆ˜
def normalize_relative(X):
    X_normalized = []
    for row in X:
        lx0, ly0 = row[0], row[21]   # ì™¼ì† ê¸°ì¤€ì 
        rx0, ry0 = row[42], row[63]  # ì˜¤ë¥¸ì† ê¸°ì¤€ì 

        lx = row[0:21] - lx0
        ly = row[21:42] - ly0
        rx = row[42:63] - rx0
        ry = row[63:84] - ry0

        X_normalized.append(np.concatenate([lx, ly, rx, ry]))
    return np.array(X_normalized)

# âœ… ì •ê·œí™” ì ìš©
X = normalize_relative(X)
X = X.reshape(-1, 84, 1)  # Conv1D ì…ë ¥ í˜•ì‹

# ğŸ·ï¸ ë¼ë²¨ ì²˜ë¦¬
y = df["label"].values
le = LabelEncoder()
y_encoded = le.fit_transform(y)
y_onehot = to_categorical(y_encoded)

# ğŸ“Š 3. í•™ìŠµ/ê²€ì¦ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y_onehot, test_size=0.2, stratify=y_encoded, random_state=42
)

# ğŸ§± 4. ëª¨ë¸ êµ¬ì„±
model = Sequential([
    Conv1D(128, kernel_size=5, activation='relu', input_shape=(84, 1)),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    Conv1D(256, kernel_size=3, activation='relu'),
    BatchNormalization(),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    LSTM(128, return_sequences=False),
    Dense(512, activation='relu'),
    Dropout(0.4),

    Dense(y_onehot.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# ğŸš€ 5. í•™ìŠµ
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=30,
    batch_size=64,
    callbacks=[early_stop]
)

# ğŸ“ˆ 6. í‰ê°€ ë° ì €ì¥
loss, acc = model.evaluate(X_test, y_test)
print(f"âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {acc:.4f}")

model.save(MODEL_SAVE_PATH)
joblib.dump(le, ENCODER_SAVE_PATH)
print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MODEL_SAVE_PATH}")
print(f"ğŸ’¾ ë¼ë²¨ ì¸ì½”ë” ì €ì¥ ì™„ë£Œ: {ENCODER_SAVE_PATH}")