import os
import cv2
import numpy as np
import mediapipe as mp
from collections import deque
from tensorflow.keras.models import load_model
from PIL import Image, ImageFont, ImageDraw

FPS = 30                        # ì›¹ìº  í”„ë ˆì„ ì†ë„ì— ë§ì¶° ì¡°ì •
threshold_lost   = FPS * 2      # ì† ì‚¬ë¼ì§„ í›„ 2ì´ˆ(ì—°ì† í”„ë ˆì„ ìˆ˜)
threshold_display = FPS * 4     # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ 4ì´ˆê°„ ìœ ì§€í•  í”„ë ˆì„ ìˆ˜

# ì† ê°ì§€ ì „/í›„ ìƒíƒœ í”Œë˜ê·¸
hand_was_detected = False      # ì§ì „ í”„ë ˆì„ì— ì†ì´ ìˆì—ˆëŠ”ì§€
gesture_active = False         # í˜„ì¬ ì œìŠ¤ì²˜ ìˆ˜ì§‘ ì¤‘ì¸ì§€

lost_count = 0                  # ì† ì‚¬ë¼ì§„ í”„ë ˆì„ ì¹´ìš´í„°
display_mode = False            # ê²°ê³¼ ìœ ì§€ ëª¨ë“œ í”Œë˜ê·¸
display_count = 0               # ê²°ê³¼ ìœ ì§€ í”„ë ˆì„ ì¹´ìš´í„°


# ==== ì‹œí€€ìŠ¤ ì„¤ì • ====
SEQ_NAME = "L20"  # â† 'L10', 'L20', 'L30', 'L40' ì¤‘ í…ŒìŠ¤íŠ¸í•  ì‹œí€€ìŠ¤ ì§€ì •
WINDOW_SIZE = int(SEQ_NAME[1:])
CONF_THRESH = 0.3

# ==== ê²½ë¡œ ì„¤ì • ====
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
MODEL_DIR = os.path.join(BASE_DIR, 'models', SEQ_NAME)

model = load_model(os.path.join(MODEL_DIR, 'sign_language_model_normalized.h5'))
label_classes = np.load(os.path.join(MODEL_DIR, 'label_classes.npy'), allow_pickle=True)
X_mean = np.load(os.path.join(MODEL_DIR, 'X_mean.npy'))
X_std = np.load(os.path.join(MODEL_DIR, 'X_std.npy'))
id2label = {i: lbl for i, lbl in enumerate(label_classes)}

# ==== MediaPipe ì„¤ì • ====
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.7, min_tracking_confidence=0.7)
mp_drawing = mp.solutions.drawing_utils

# ==== ìƒíƒœ ====
sequence = deque()
collecting = False
latest_text = ""

font = ImageFont.truetype("/System/Library/Fonts/Supplemental/AppleGothic.ttf", 32)

def draw_text(img, text, pos=(10,50), color=(255,255,0)):
    pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(pil)
    draw.text(pos, text, font=font, fill=color)
    return cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)

def extract_rel(lms, W, H):
    if not lms:
        return [0]*42
    pts = [(p.x*W, p.y*H) for p in lms]
    bx, by = pts[0]
    rel = []
    for x, y in pts:
        rel += [ x-bx, y-by ]
    return rel


def calc_ang(lms):
    if not lms:
        return [0]*15
    ang = []
    for i in range(len(lms)-2):
        a = np.array([lms[i].x, lms[i].y])
        b = np.array([lms[i+1].x, lms[i+1].y])
        c = np.array([lms[i+2].x, lms[i+2].y])
        ba = a - b
        bc = c - b
        cos = np.dot(ba, bc) / (np.linalg.norm(ba)*np.linalg.norm(bc)+1e-6)
        ang.append(np.degrees(np.arccos(np.clip(cos, -1, 1))))
    return ang[:15] + [0]*(15 - len(ang))

# ==== ì›¹ìº  ì‹œì‘ ====
cap = cv2.VideoCapture(0)
cv2.namedWindow("Sign2Text ì‹¤ì‹œê°„ ì¸ì‹", cv2.WINDOW_NORMAL)
print(f"[{SEQ_NAME}] s=ìˆ˜ì§‘ ì‹œì‘/ì¤‘ì§€, q=ì¢…ë£Œ")

while True:
    ret, frame = cap.read()
    if not ret:
        continue

    img = cv2.flip(frame, 1)
    H, W = img.shape[:2]
    results = hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

    # 1) ì† ê°ì§€ ì—¬ë¶€ ì—…ë°ì´íŠ¸
    hand_detected = bool(results.multi_hand_landmarks)

    # 2) ìƒˆë¡œìš´ ì œìŠ¤ì²˜ ì‹œì‘ ê°ì§€ (së¥¼ ëˆŒëŸ¬ ìˆ˜ì§‘ ì‹œì‘í•˜ì˜€ì„ ë•Œì—ë§Œ)
    if collecting and hand_detected and not hand_was_detected:
        gesture_active = True
        sequence.clear()
        print("ğŸ”˜ ì œìŠ¤ì²˜ ì‹œì‘ ê°ì§€")

    # 3) ì´ì „ ì† ê°ì§€ ìƒíƒœ ê°±ì‹ 
    hand_was_detected = hand_detected

    # 4) ì†ì´ ì‚¬ë¼ì§„ ì—°ì† í”„ë ˆì„ ì„¸ê¸° (ì œìŠ¤ì²˜ ì¤‘ì¼ ë•Œë§Œ)
    if gesture_active:
        if not hand_detected:
            lost_count += 1
        else:
            lost_count = 0


    if collecting and display_mode and len(sequence)>0:
        display_mode = False

    left, right = [], []
    if results.multi_hand_landmarks:
        for lm, hd in zip(results.multi_hand_landmarks, results.multi_handedness):
            if hd.classification[0].label == 'Left':  left = lm.landmark
            else:                                      right = lm.landmark
            mp_drawing.draw_landmarks(img, lm, mp_hands.HAND_CONNECTIONS)

    feats = extract_rel(left, W, H) + extract_rel(right, W, H) + calc_ang(left) + calc_ang(right)
    if collecting and sum(abs(f) for f in feats):
        sequence.append(feats)


    # ìë™ ì˜ˆì¸¡ íŠ¸ë¦¬ê±°
    if collecting and gesture_active and lost_count >= threshold_lost and not display_mode:
        # ìœˆë„ìš° í•˜ë‚˜ë¼ë„ ì•ˆ ì±„ì›Œì¡Œìœ¼ë©´ í¬ê¸°
        if len(sequence) >= WINDOW_SIZE:
            seq_arr = np.array(sequence)
            windows = np.stack([
                seq_arr[i:i+WINDOW_SIZE]
                for i in range(len(seq_arr)-WINDOW_SIZE+1)
            ], axis=0)
            normed = (windows - X_mean) / X_std
            preds = model.predict(normed, verbose=0)
            win_idx = preds.max(axis=1).argmax()
            best_pred = preds[win_idx]
            class_idx = best_pred.argmax()
            best_conf = best_pred[class_idx]

            if best_conf > CONF_THRESH:
                latest_text = f"{id2label[class_idx]} ({best_conf:.2f})"
                print("âœ… ì˜ˆì¸¡:", latest_text)
            else:
                latest_text = ""
                print(f"â— ì‹ ë¢°ë„ ë¶€ì¡±: {best_conf:.2f}")

            display_mode = True
            display_count = 0

        # ì œìŠ¤ì²˜ í•œ ì‚¬ì´í´ ë
        gesture_active = False
        lost_count = 0

    # ê²°ê³¼ ìœ ì§€ ëª¨ë“œ: 5ì´ˆê°„ ê²°ê³¼ í‘œì‹œ í›„ ì´ˆê¸°í™”
    if display_mode:
        display_count += 1
        if display_count >= threshold_display:
            display_mode = False
            latest_text = ""
            sequence.clear()


    img = draw_text(img, f"seq_len={len(sequence)} / {WINDOW_SIZE}", (10, 50))
    if latest_text:
        img = draw_text(img, f"ê²°ê³¼: {latest_text}", (10, 100))

    cv2.imshow("Sign2Text ì‹¤ì‹œê°„ ì¸ì‹", img)
    key = cv2.waitKey(1) & 0xFF

    if key == ord('q'):
        break
    elif key == ord('s'):
        collecting = not collecting
        if collecting:
            sequence.clear()
            latest_text = ""
            print("ğŸ”˜ ìˆ˜ì§‘ ì‹œì‘")
        else:
            print("ğŸ”˜ ìˆ˜ì§‘ ì¤‘ì§€")

cap.release()
cv2.destroyAllWindows()