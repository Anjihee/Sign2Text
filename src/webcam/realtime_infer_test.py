import os
import time
import cv2
import numpy as np
import mediapipe as mp
from collections import deque
from tensorflow.keras.models import load_model
from PIL import Image, ImageFont, ImageDraw
import platform

# ==== ì‹œí€€ìŠ¤ ì„¤ì • ====
SEQ_NAME    = "L20"                  # ì‚¬ìš©í•  ì‹œí€€ìŠ¤ ì´ë¦„ (ì˜ˆ: L10, L20, L30â€¦)
WINDOW_SIZE = int(SEQ_NAME[1:])      # ì‹œí€€ìŠ¤ ê¸¸ì´ (ìˆ«ìž ë¶€ë¶„)
CONF_THRESH = 0.3                    # ì˜ˆì¸¡ í—ˆìš© ì‹ ë¢°ë„ ë¬¸í„±ê°’
T           = 2.5                    # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°

# ==== ê²½ë¡œ ì„¤ì • ====
BASE_DIR      = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
MODEL_DIR     = os.path.join(BASE_DIR, 'models', SEQ_NAME)
model         = load_model(os.path.join(MODEL_DIR, 'sign_language_model_normalized.h5'))
label_classes = np.load(os.path.join(MODEL_DIR, 'label_classes.npy'), allow_pickle=True)
X_mean        = np.load(os.path.join(MODEL_DIR, 'X_mean.npy'))
X_std         = np.load(os.path.join(MODEL_DIR, 'X_std.npy'))
id2label      = {i: lbl for i, lbl in enumerate(label_classes)}

# ==== MediaPipe ì„¤ì • ====
mp_hands   = mp.solutions.hands
hands      = mp_hands.Hands(max_num_hands=2,
                            min_detection_confidence=0.7,
                            min_tracking_confidence=0.7)
mp_drawing = mp.solutions.drawing_utils

# ==== í°íŠ¸ ì„¤ì • ====
try:
    if platform.system() == "Windows":
        font_path = "C:/Windows/Fonts/malgun.ttf"
    else:
        font_path = "/System/Library/Fonts/Supplemental/AppleGothic.ttf"
    font = ImageFont.truetype(font_path, 32)
except:
    font = ImageFont.load_default()

def draw_text(img, text, pos=(10, 50), color=(255,255,0)):

    """OpenCV ì´ë¯¸ì§€ ìœ„ì— í•œê¸€ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°."""
    pil  = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(pil)
    draw.text(pos, text, font=font, fill=color)
    return cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)

def extract_rel(lms, W, H):
    """ëžœë“œë§ˆí¬ ì¢Œí‘œë¥¼ base point ëŒ€ë¹„ ìƒëŒ€ì¢Œí‘œë¡œ ë³€í™˜."""
    if not lms:
        return [0]*42
    pts = [(p.x*W, p.y*H) for p in lms]
    bx, by = pts[0]
    rel = []
    for x, y in pts:
        rel += [x - bx, y - by]
    return rel

def calc_ang(lms):
    """ëžœë“œë§ˆí¬ë“¤ ê°„ì˜ ê´€ì ˆ ê°ë„ ê³„ì‚° (ìµœëŒ€ 15ê°œ)."""
    if not lms:
        return [0]*15
    ang = []
    for i in range(len(lms)-2):
        a  = np.array([lms[i].x, lms[i].y])
        b  = np.array([lms[i+1].x, lms[i+1].y])
        c  = np.array([lms[i+2].x, lms[i+2].y])
        ba = a - b
        bc = c - b
        cos = np.dot(ba, bc) / (np.linalg.norm(ba)*np.linalg.norm(bc) + 1e-6)
        ang.append(np.degrees(np.arccos(np.clip(cos, -1,1))))
    # 15ê°œ ê³ ì • í¬ê¸°ë¡œ íŒ¨ë”©
    return ang[:15] + [0]*(15 - len(ang))

# ==== ì›¹ìº  ì´ˆê¸°í™” ====
cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
if not cap.isOpened():
    cap.release()
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ì›¹ìº  ì—´ê¸° ì‹¤íŒ¨")
        exit(1)
cap.set(cv2.CAP_PROP_FRAME_WIDTH,  640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

cv2.namedWindow("Sign2Text", cv2.WINDOW_NORMAL)
print(f"[{SEQ_NAME}] s=ìˆ˜ì§‘ ì‹œìž‘/ì¤‘ì§€, q=ì¢…ë£Œ")

# ==== ìƒíƒœ ë³€ìˆ˜ ====
sequence       = deque()
collecting     = False
latest_text    = ""
gesture_active = False
hand_was_detected = False
hands_gone_at  = None
display_mode   = False
display_timer  = 0

# ==== ë©”ì¸ ë£¨í”„ ====
while True:
    ret, frame = cap.read()
    if not ret:
        continue

    img = cv2.flip(frame, 1)
    H, W = img.shape[:2]
    results = hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

    # ì† ê°ì§€ ì—¬ë¶€
    hand_detected = bool(results.multi_hand_landmarks)

    # ìˆ˜ì§‘ í† ê¸€/ì¢…ì§€
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break

    elif key == ord('s'):
        collecting = not collecting
        if collecting:
            sequence.clear()
            latest_text = ""
            print("ðŸ”˜ ìˆ˜ì§‘ ì‹œìž‘")
        else:
            print("ðŸ”˜ ìˆ˜ì§‘ ì¤‘ì§€")

    # â‘  ì œìŠ¤ì²˜ ì‹œìž‘ ê°ì§€ (ìˆ˜ì§‘ ì¤‘ + ì† ì²« ì§„ìž…)
    if collecting and hand_detected and not hand_was_detected:
        gesture_active = True
        sequence.clear()
        hands_gone_at = None
        print("ðŸ”˜ ì œìŠ¤ì²˜ ì‹œìž‘")

    hand_was_detected = hand_detected

    # ëžœë“œë§ˆí¬ ê·¸ë¦¬ê¸° & íŠ¹ì§• ìˆ˜ì§‘
    left, right = [], []
    if results.multi_hand_landmarks:
        for lm, hd in zip(results.multi_hand_landmarks, results.multi_handedness):
            if hd.classification[0].label == 'Left':
                left = lm.landmark
            else:
                right = lm.landmark
            mp_drawing.draw_landmarks(img, lm, mp_hands.HAND_CONNECTIONS)


    feats = extract_rel(left, W, H) + extract_rel(right, W, H) + calc_ang(left) + calc_ang(right)
    if collecting and any(abs(f)>1e-6 for f in feats):
        sequence.append(feats)

    # â‘¡ ì œìŠ¤ì²˜ ì¤‘ ì† ì‚¬ë¼ì§ ê¸°ë¡
    if gesture_active:
        if not hand_detected:
            if hands_gone_at is None:
                hands_gone_at = time.time()
        else:
            hands_gone_at = None

    # â‘¢ 2s í›„ ìžë™ ì˜ˆì¸¡ íŠ¸ë¦¬ê±°
    if collecting and gesture_active and hands_gone_at and time.time() - hands_gone_at >= 2.0 and not display_mode:
        seq_arr   = np.array(sequence, dtype=np.float32)
        n_windows = len(seq_arr) - WINDOW_SIZE + 1
        windows   = np.stack([seq_arr[i:i+WINDOW_SIZE] for i in range(n_windows)], axis=0)
        normed    = (windows - X_mean) / X_std
        preds     = model.predict(normed, verbose=0)

        # ì˜¨ë„ ìŠ¤ì¼€ì¼ë§
        logits  = np.log(np.clip(preds, 1e-12, 1.0))
        scaled  = np.exp(logits / T)
        preds_T = scaled / np.sum(scaled, axis=1, keepdims=True)

        # ì°½ë³„ ìµœê³ ì  -> Top-3
        window_scores = preds_T.max(axis=1)
        best_win      = window_scores.argmax()
        best_pred     = preds_T[best_win]
        top3          = best_pred.argsort()[-3:][::-1]

        print("=== Top 3 ===")
        for idx in top3:
            print(f"{id2label[idx]}: {best_pred[idx]:.2f}")
        print("=============")

        # ë¬¸í„±ê°’ íŒì •
        top1_conf = best_pred[top3[0]]
        if top1_conf > CONF_THRESH:
            latest_text = f"{id2label[top3[0]]} ({top1_conf:.2f})"
            print("âœ… ì˜ˆì¸¡:", latest_text)
        else:
            latest_text = ""
            print(f"â— ì‹ ë¢°ë„ ë¶€ì¡±: {top1_conf:.2f}")

        sequence.clear()
        gesture_active = False
        hands_gone_at  = None
        # ê²°ê³¼ ì¶œë ¥ ìœ ì§€ ì‹œìž‘
        display_mode  = True
        display_timer = time.time()

    # â‘¤ ê²°ê³¼ 4ì´ˆ ìœ ì§€
    if display_mode and time.time() - display_timer >= 4.0:
        display_mode = False
        latest_text  = ""

    # í™”ë©´ í‘œì‹œ
    status = f"{'ìˆ˜ì§‘ ì¤‘' if collecting else 'ëŒ€ê¸° ì¤‘'} | len={len(sequence)}/{WINDOW_SIZE}"
    img = draw_text(img, status, pos=(10,50))
    if latest_text:
        img = draw_text(img, f"ê²°ê³¼: {latest_text}", pos=(10,100))

    cv2.imshow("Sign2Text", img)

# ì •ë¦¬
cap.release()
cv2.destroyAllWindows()
